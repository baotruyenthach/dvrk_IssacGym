Sample RL Framework
####################

Overview
=========

We provide a simple Reinforcement Learning framework that bridges simulation with RL. 
As part of Isaac Gym, we include a minimal PyTorch impelementation of PPO, **rl-pytorch**, which can be used to train our sample tasks.
In addition, we have more advanced examples of training with a third-party highly-optimized RL library, `rl_games <https://github.com/Denys88/rl_games>`_. This also demonstrates how our framework can be used with other RL libraries.

To use **rl_games** the following instructions should be performed:

    git clone https://github.com/Denys88/rl_games.git
    git checkout tags/v1.0-alpha2
    pip install -e .

For all the sample tasks provided, we include training configurations for both rl-pytorch and rl_games, denoted with prefixes ``pytorch_ppo_*.yaml`` and ``rlg_*.yaml``. These files are located in python/rlgpu/cfg. The appropriate config file will be selected automatically based on the task being executed and the script that it is being launched from. To launch a task using rl-pytorch, run ``python train.py``, with rl_games, run ``python rlg_train.py`` from python/rlgpu.

List of Sample Tasks
=====================
.. cartpole.py:

Cartpole (cartpole.py)
----------------------
Cartpole is a simple example that shows usage of the DOF state tensors. Position and velocity data are used as observation for the cart and pole DOFs. Actions are applied as forces to the cart using ``set_dof_actuation_force_tensor``. During reset, we use ``set_dof_state_tensor_indexed`` to set DOF position and velocity of the cart and pole to a randomized state.

Config files used for this task are:

- **Task config**: cartpole.yaml
- **rl-pytorch training config**: pytorch_ppo_cartpole.yaml
- **rl_games training config**: rlg_base.yaml

.. ball_balance.py:

Ball Balance (ball_balance.py)
------------------------------
The Ball Balance task is a great example to showcase the use of force and torque sensors, as well as DOF states for the table and root states for the ball. In this example, the three-legged table has a force sensor attached to each leg using the ``create_force_sensor`` API. We use the force sensor tensor APIs to collect force and torque data on the legs, which guide position target ouputs produced by the policy. The example shows usage of ``set_dof_position_target_tensor`` to set position targets to keep the ball balanced on the table. 

Config files used for this task are:

- **Task config**: ball_balance.yaml
- **rl-pytorch training config**: pytorch_ppo_ball_balance.yaml
- **rl_games training config**: rlg_base.yaml

.. franka.py:

Franka Drawer Opening (franka.py)
---------------------------------
The Franka example demonstrates interaction between Franka arm and cabinet, as well as setting states of objects inside the drawer. 
It also showcases control of the Franka arm using position targets. 
In this example, we use DOF state tensors to retrieve the state of the Franka arm, as well as the state of the drawer on the cabinet.
Actions are applied using ``set_dof_position_target_tensor`` to set position targets for the Franka arm DOFs.

During reset, we use indexed versions of APIs to reset Franka, cabinet, and objects inside drawer to their initial states. ``set_actor_root_state_tensor_indexed`` is used to reset objects inside drawer, ``set_dof_position_target_tensor_indexed`` is used to reset Franka, and ``set_dof_state_tensor_indexed`` is used to reset Franka and cabinet.

Config files used for this task are:

- **Task config**: franka_cabinet.yaml
- **rl-pytorch training config**: pytorch_ppo_franka_cabinet.yaml
- **rl_games training config**: rlg_franka_cabinet.yaml

.. ant.py:

Ant (ant.py)
------------
An example of a simple locomotion task, the goal is to train quadruped robots (ants) to run forward as fast as possible.
The Ant task includes examples of utilizing Isaac Gym's actor root state tensor, DOF state tensor, and force sensor tensor APIs.
Actor root states provide data for the ant's root body, including position, rotation, linear and angular velocities. This information can be used to detect whether the ant has been moving towards the desired direction and whether it has fallen or flipped over.
DOF states are used to retrieve the position and velocity of each DOF for the ant, and force sensors are used to indicate contacts with the ground plane on the ant's legs.

Actions are applied onto the DOFs of the ants to allow it to move, using the ``set_dof_actuation_force_tensor`` API.

During resets, we also show usage of ``set_actor_root_state_tensor_indexed`` and ``set_dof_state_tensor_indexed`` APIs for setting select ants into a valid starting state. 

Config files used for this task are:

- **Task config**: ant.yaml
- **rl-pytorch training config**: pytorch_ppo_ant.yaml
- **rl_games training config**: rlg_ant.yaml

.. humanoid.py

Humanoid (humanoid.py)
----------------------
The humanoid example is conceptually very similar to the Ant task. 
In this example, we also use actor root states to detect whether humanoids are been moving towards the desired direction and whether they have fallen.
DOF states are used to retrieve the position and velocity of each DOF for the humanoids, and force sensors are used to indicate contacts with the ground plane on the humanoids' feet.

Config files used for this task are:

- **Task config**: humanoid.yaml
- **rl-pytorch training config**: pytorch_ppo_humanoid.yaml
- **rl_games training config**: rlg_humanoid.yaml

.. shadow_hand.py

Shadow Hand Object Manipulation (shadow_hand.py)
------------------------------------------------
The Shadow Hand task is an example of a challenging dexterity manipulation task with complex contact dynamics.
It also demonstrates the use of tendons in the Shadow Hand model.
In this example, we use ``get_asset_tendon_properties`` and ``set_asset_tendon_properties`` to get and set tendon properties for the hand.
Motion of the hand is controlled using position targets with ``set_dof_position_target_tensor``.

The goal is to orient the object in the hand to match the target orientation. There is a goal object that shows the target orientation to be achieved by the manipulated object. 
To reset both the target object and the object in hand, it is important to make **one** single call to ``set_actor_root_state_tensor_indexed`` to set the states for both objects.
This task has 3 difficulty levels using different objects to manipulate - block, egg and pen and different observations schemes - ``dof``, ``dof_no_vel``, ``fingertip``,
``full`` and ``full_force_sensor`` that can be set in the task config in ``observationType`` field.

Config files used for this task are:

- **Task config**: shadow_hand.yaml
- **rl-pytorch training config**: pytorch_ppo_shadow_hand.yaml
- **rl_games training config**: rlg_shadow_hand.yaml

Observations types:

- **dof**: a standard set of observations with joint positions and velocities, object pose, linear and angular velocities and the goal pose
- **dof_no_vel**: the same as ``dof`` but without any velocity information for joints and object 
- **fingertip**: similar to ``dof``, but instead of joint states, fingertip transforms, linear and angular velocities are provided
- **full**: ``dof`` set of observations plus fingertip transforms, and their linear and angular velocities
- **full_force_sensor**: ``full`` set of observations plus readings from force-torque sensors attached to the fingertips

Class Definition
================

There are two base classes defined for Isaac Gym's RL framework: ``base_task.py`` and ``vec_task.py``.
These are located in python/tasks/base and are the fundamental core of the RL framework.

BaseTask Class (base_task.py)
-----------------------------

The BaseTask class is designed to act as a parent class for all RL tasks using Isaac Gym's RL framework.
It provides an interface for interaction with RL alrogithms and includes functionalities that are required for all RL tasks.

BaseTask constructor takes a few arguments:

num_obs
    Number of observations for the task
num_acts
    Number of actions for the task
num_envs
    Number of environments in simulation
graphics_device
    Device to use for graphical display
device
    Device to use for simulation and task

The ``__init__`` function of BaseTask initializes buffers required for RL on the device specified. These include observation buffer, reward buffer, reset buffer, progress buffer, randomization buffer, and an optional extras array for passing in any additional information to the RL algorithm. This function will then trigger a call to ``create_sim()``, which must be implemented by the extended classes. A call to ``prepare_sim()`` will also be made to initialize the internal data structures for simulation. If running with a viewer, this function will also initialize the viewer and create keyboard shortcuts for quitting the application (ESC) and disabling/enabling rendering (V). 

The ``step`` function is designed to guide the workflow of each RL iteration. This function can be viewed in three parts: ``pre_physics_step``, ``simulate``, and ``post_physics_step``. ``pre_physics_step`` should be implemented to perform any computations required before stepping the physics simulation. As an example, applying actions from the policy should happen in ``pre_physics_step``. ``simulate`` is then called to step the physics simulation. ``post_physics_step`` should implement computations performed after stepping the physics simulation, e.g. computing rewards and observations.

BaseTask also provides an implementation of ``render`` to step graphics if a viewer is initialized.

Additionally, BaseTask provides an interface to perform Domain Randomization via the ``apply_randomizations`` method. For more details, please see :doc:`Domain Randomization <domainrandomization>`.

VecTask Class (vec_task.py)
---------------------------
VecTask provides a vectorized wrapper of the task to interact directly with RL algorithms. When a task is executed, we wrap the specified task class in a VecTask class and pass this wrapper to the RL algorithm. Implementation details can be found in python/rlgpu/utils/launch_task.py.

VecTask constructor takes a few argumets:

task
    Task instance to be executed (inherited from BaseTask)
rl_device
    Device to use for RL algorithm
clip_observations
    Observations will be clipped to the range [-clip_observation, clip_observations]
clip_actions
    Actions will be clipped to the range [-clip_actions, clip_actions]

We provide three classes inherited from VecTask: ``VecTaskPython``, ``VecTaskCPU`` and ``VecTaskGPU``.

``VecTaskPython`` is used for all python tasks extended from BaseTask. This class implements the ``step`` method, which sends clipped actions from the RL algorithm to the task, triggers task's ``step`` method, and sends back clipped observation buffer, reward buffer, reset buffer, and extras to the RL algorithm. This class also implements a ``reset`` method that steps the task with a close-to-zero action buffer and provides RL algorithm with an updated clipped observation buffer. This implementation can be modified based on needs of the task and RL library.

``VecTaskCPU`` and ``VecTaskGPU`` are both designed to support C++ implementations. ``VecTaskGPU`` in particular is designed to work with CUDA implementations. Both classes implement ``step`` and ``reset`` functions that behave in similar fashion as ``VecTaskPython``. 


Creating a New Task
====================

Creating a new task is straight-forward using Isaac Gym's RL framework. The first step is to create a new script file in python/rlgpu/tasks.

To use Isaac Gym's APIs, we need the following imports

.. code-block:: python

    from rlgpu.tasks.base.base_task import BaseTask
    from isaacgym import gymtorch
    from isaacgym import gymapi

Then, we need to create a Task class that extends from BaseTask

.. code-block:: python

    class MyNewTask(BaseTask):

In the ``__init__`` method of MyNewTask, make sure to make a call to BaseTask's ``__init__`` to initialize the simulation

.. code-block:: python

    super().__init__(
        num_obs=num_obs,
        num_acts=num_acts,
        num_envs=num_envs,
        graphics_device=graphics_device,
        device=device
    )

Then, we can initialize state tensors that we may need for our task. For example, we can initialize the DOF state tensor

.. code-block:: python

    dof_state_tensor = self.gym.acquire_dof_state_tensor(self.sim)
    self.dof_state = gymtorch.wrap_tensor(dof_state_tensor)

There are a few methods that must be implemented by a child class of BaseTask: ``create_sim``, ``pre_physics_step``, ``post_physics_step``.

.. code-block:: python

    def create_sim(self):
        # implement sim set up and environment creation here
        #    - set up-axis
        #    - call self.gym.create_sim
        #    - create ground plane
        #    - set up environments

    def pre_physics_step(self, actions):
        # implement pre-physics simulation code here
        #    - e.g. apply actions

    def post_physics_step(self):
        # implement post-physics simulation code here
        #    - e.g. compute reward, compute observations

To launch the new task from ``train.py`` or ``rlg_train.py``, add your new task to the imports in python/rlgpu/utils/launch_task.py

.. code-block:: python

    from rlgpu.task.my_new_task import MyNewTask

To automatically load in task and training config files, add your task name to python/rlgpu/utils/config.py in ``retrieve_cfg(args)``.

Then, you can run your task with  ``python train.py --task MyNewTask`` or ``python rlg_train.py --task MyNewTask``
